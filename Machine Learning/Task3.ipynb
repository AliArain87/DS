{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class weights\n",
    "\n",
    "The weights from the class_weight parameter are used to train the classifier. They are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different.\n",
    "\n",
    "Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.\n",
    "How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it.\n",
    "\n",
    "# The metrics\n",
    "Once you have a classifier, you want to know how well it is performing. Here you can use the metrics you mentioned: accuracy, recall_score, f1_score...\n",
    "\n",
    "Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.\n",
    "\n",
    "I will not detail all these metrics but note that, with the exception of accuracy, they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. They rely on concepts such as true positives or false negative that require defining which class is the positive one.\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.65      1.00      0.79        17\n",
    "          1       0.57      0.75      0.65        16\n",
    "          2       0.33      0.06      0.10        17\n",
    "avg / total       0.52      0.60      0.51        50\n",
    "# The warning\n",
    "F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The \n",
    "default `weighted` averaging is deprecated, and from version 0.18, \n",
    "use of precision, recall or F-score with multiclass or multilabel data  \n",
    "or pos_label=None will result in an exception. Please set an explicit \n",
    "value for `average`, one of (None, 'micro', 'macro', 'weighted', \n",
    "'samples'). In cross validation use, for instance, \n",
    "scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
    "You get this warning because you are using the f1-score, recall and precision without defining how they should be computed! The question could be rephrased: from the above classification report, how do you output one global number for the f1-score? You could:\n",
    "\n",
    "Take the average of the f1-score for each class: that's the avg / total result above. It's also called macro averaging.\n",
    "Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka micro averaging.\n",
    "Compute a weighted average of the f1-score. Using 'weighted' in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.\n",
    "These are 3 of the options in scikit-learn, the warning is there to say you have to pick one. So you have to specify an average argument for the score method.\n",
    "\n",
    "Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5.\n",
    "\n",
    "The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.\n",
    "\n",
    "# Computing scores\n",
    "Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen. This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.\n",
    "\n",
    "Here's a way to do it using StratifiedShuffleSplit, which gives you a random splits of your data (after shuffling) that preserve the label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
